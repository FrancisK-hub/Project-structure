{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfaaba91",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Project structure\n",
    "\n",
    "Codes\n",
    "\n",
    "```text\n",
    "batch_ml_platform/\n",
    "  config/\n",
    "    config.yaml\n",
    "  data/\n",
    "    bronze/\n",
    "      operational/\n",
    "      logs/\n",
    "      external/\n",
    "    silver/\n",
    "      cleaned/\n",
    "    gold/\n",
    "      features/\n",
    "      labels/\n",
    "      training_sets/\n",
    "  src/\n",
    "    __init__.py\n",
    "    config_loader.py\n",
    "    utils.py\n",
    "    bronze_ingestion.py\n",
    "    silver_processing.py\n",
    "    gold_feature_store.py\n",
    "    run_quarterly_pipeline.py\n",
    "  requirements.txt\n",
    "  README.md\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Example `requirements.txt`\n",
    "Install Packages \n",
    "\n",
    "```text\n",
    "pyspark==3.5.0\n",
    "pyyaml==6.0.2\n",
    "pandas==2.2.2\n",
    "pyarrow==17.0.0\n",
    "```\n",
    "\n",
    "Install with:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. `config/config.yaml`\n",
    "\n",
    "This centralizes paths and basic settings for the quarterly batch:\n",
    "\n",
    "```yaml\n",
    "storage:\n",
    "  base_path: \"./data\"\n",
    "  bronze_path: \"./data/bronze\"\n",
    "  silver_path: \"./data/silver\"\n",
    "  gold_path: \"./data/gold\"\n",
    "\n",
    "sources:\n",
    "  operational:\n",
    "    path: \"./data/raw_sources/operational\"\n",
    "    pattern: \"*.csv\"\n",
    "  logs:\n",
    "    path: \"./data/raw_sources/logs\"\n",
    "    pattern: \"*.csv\"\n",
    "  external:\n",
    "    path: \"./data/raw_sources/external\"\n",
    "    pattern: \"*.csv\"\n",
    "\n",
    "batch:\n",
    "  training_quarter: \"2025-Q4\"\n",
    "  cutoff_date: \"2025-09-30\"\n",
    "\n",
    "spark:\n",
    "  app_name: \"Quarterly_Batch_Pipeline\"\n",
    "  master: \"local[*]\"\n",
    "\n",
    "governance:\n",
    "  enable_lineage_logging: true\n",
    "  lineage_log_path: \"./data/lineage/lineage_log.csv\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. `src/config_loader.py`\n",
    "\n",
    "Loads config from YAML.\n",
    "\n",
    "```python\n",
    "# src/config_loader.py\n",
    "# Load YAML configuration for the batch data platform.\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "def load_config(config_path=\"config/config.yaml\"):\n",
    "    if not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(\"Config file not found at path \" + config_path)\n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. `src/utils.py`\n",
    "\n",
    "Spark session, lineage logging, quality checks.\n",
    "\n",
    "```python\n",
    "# src/utils.py\n",
    "# Shared utilities: Spark session factory, lineage logging, simple DQ helpers.\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, lit\n",
    "\n",
    "def create_spark_session(app_name, master=\"local[*]\"):\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .appName(app_name)\n",
    "        .master(master)\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    return spark\n",
    "\n",
    "def log_lineage(config, step_name, input_paths, output_path):\n",
    "    if not config.get(\"governance\", {}).get(\"enable_lineage_logging\", False):\n",
    "        return\n",
    "\n",
    "    lineage_path = config[\"governance\"][\"lineage_log_path\"]\n",
    "    os.makedirs(os.path.dirname(lineage_path), exist_ok=True)\n",
    "\n",
    "    timestamp = datetime.datetime.utcnow().isoformat()\n",
    "\n",
    "    record = {\n",
    "        \"timestamp_utc\": timestamp,\n",
    "        \"step_name\": step_name,\n",
    "        \"input_paths\": \";\".join(input_paths),\n",
    "        \"output_path\": output_path,\n",
    "    }\n",
    "\n",
    "    if os.path.exists(lineage_path):\n",
    "        df = pd.read_csv(lineage_path)\n",
    "        df = pd.concat([df, pd.DataFrame([record])], ignore_index=True)\n",
    "    else:\n",
    "        df = pd.DataFrame([record])\n",
    "\n",
    "    df.to_csv(lineage_path, index=False)\n",
    "\n",
    "def dq_null_check(df, key_columns):\n",
    "    checks = []\n",
    "    for c in key_columns:\n",
    "        null_cnt = df.filter(col(c).isNull()).count()\n",
    "        checks.append((c, null_cnt))\n",
    "    return checks\n",
    "\n",
    "def dq_duplicate_check(df, key_columns):\n",
    "    dup_count = (\n",
    "        df.groupBy([col(c) for c in key_columns])\n",
    "        .agg(count(\"*\").alias(\"cnt\"))\n",
    "        .filter(col(\"cnt\") > 1)\n",
    "        .count()\n",
    "    )\n",
    "    return dup_count\n",
    "\n",
    "def add_batch_metadata(df, batch_id, source_name):\n",
    "    df_with_meta = df.withColumn(\"batch_id\", lit(batch_id)).withColumn(\"source_system\", lit(source_name))\n",
    "    return df_with_meta\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Bronze layer ingestion – `src/bronze_ingestion.py`\n",
    "\n",
    "```python\n",
    "# src/bronze_ingestion.py\n",
    "# Batch ingestion into bronze: append-only raw storage for each source.\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from pyspark.sql.functions import input_file_name\n",
    "\n",
    "from config_loader import load_config\n",
    "from utils import create_spark_session, log_lineage, add_batch_metadata\n",
    "\n",
    "def ingest_source_to_bronze(spark, config, source_name, batch_id):\n",
    "    source_cfg = config[\"sources\"][source_name]\n",
    "    src_path = source_cfg[\"path\"]\n",
    "    src_pattern = source_cfg[\"pattern\"]\n",
    "\n",
    "    files = glob.glob(os.path.join(src_path, src_pattern))\n",
    "    if len(files) == 0:\n",
    "        print(\"No files found for source \" + source_name + \" in path \" + src_path)\n",
    "        return None\n",
    "\n",
    "    df = (\n",
    "        spark.read\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .csv(files)\n",
    "        .withColumn(\"ingest_file_path\", input_file_name())\n",
    "    )\n",
    "\n",
    "    df = add_batch_metadata(df, batch_id, source_name)\n",
    "\n",
    "    bronze_base = config[\"storage\"][\"bronze_path\"]\n",
    "    target_path = os.path.join(bronze_base, source_name)\n",
    "\n",
    "    (\n",
    "        df.write\n",
    "        .mode(\"append\")\n",
    "        .option(\"compression\", \"snappy\")\n",
    "        .parquet(target_path)\n",
    "    )\n",
    "\n",
    "    log_lineage(config, \"bronze_ingestion_\" + source_name, files, target_path)\n",
    "\n",
    "    print(\"Ingested source \" + source_name + \" into bronze at \" + target_path)\n",
    "    return target_path\n",
    "\n",
    "def run_bronze_ingestion():\n",
    "    config = load_config()\n",
    "    spark = create_spark_session(\n",
    "        config[\"spark\"][\"app_name\"] + \"_bronze\",\n",
    "        master=config[\"spark\"][\"master\"],\n",
    "    )\n",
    "\n",
    "    batch_id = config[\"batch\"][\"training_quarter\"]\n",
    "\n",
    "    for src in [\"operational\", \"logs\", \"external\"]:\n",
    "        ingest_source_to_bronze(spark, config, src, batch_id)\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_bronze_ingestion()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Silver layer – cleaning and integration – `src/silver_processing.py`\n",
    "\n",
    "Applies schema enforcement, cleaning, deduplication, and joins.\n",
    "\n",
    "Assumptions:\n",
    "- Operational data: e.g. `customer_id`, `event_time`, `amount`, etc.\n",
    "- Logs: e.g. interaction logs keyed by `customer_id`.\n",
    "- External: e.g. credit scores or partner data keyed by `customer_id`.\n",
    "\n",
    "```python\n",
    "# src/silver_processing.py\n",
    "# Batch processing from bronze to silver: schema enforcement, cleaning, deduplication, integration.\n",
    "\n",
    "import os\n",
    "from pyspark.sql.functions import col, to_timestamp, trim, lower\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "\n",
    "from config_loader import load_config\n",
    "from utils import create_spark_session, log_lineage, dq_null_check, dq_duplicate_check\n",
    "\n",
    "def get_operational_schema():\n",
    "    schema = StructType(\n",
    "        [\n",
    "            StructField(\"customer_id\", StringType(), True),\n",
    "            StructField(\"event_time\", StringType(), True),\n",
    "            StructField(\"amount\", DoubleType(), True),\n",
    "            StructField(\"currency\", StringType(), True),\n",
    "            StructField(\"channel\", StringType(), True),\n",
    "        ]\n",
    "    )\n",
    "    return schema\n",
    "\n",
    "def clean_operational(spark, config, batch_id):\n",
    "    bronze_path = os.path.join(config[\"storage\"][\"bronze_path\"], \"operational\")\n",
    "    df = spark.read.parquet(bronze_path)\n",
    "\n",
    "    schema = get_operational_schema()\n",
    "    df_cast = spark.createDataFrame(df.rdd, schema)\n",
    "\n",
    "    df_clean = (\n",
    "        df_cast\n",
    "        .withColumn(\"event_time_ts\", to_timestamp(col(\"event_time\")))\n",
    "        .withColumn(\"currency\", trim(lower(col(\"currency\"))))\n",
    "        .withColumn(\"channel\", trim(lower(col(\"channel\"))))\n",
    "    )\n",
    "\n",
    "    cutoff_date = config[\"batch\"][\"cutoff_date\"]\n",
    "    df_clean = df_clean.filter(col(\"event_time_ts\") <= cutoff_date)\n",
    "\n",
    "    null_checks = dq_null_check(df_clean, [\"customer_id\", \"event_time_ts\"])\n",
    "    print(\"DQ null checks for operational: \" + str(null_checks))\n",
    "\n",
    "    dup_count = dq_duplicate_check(df_clean, [\"customer_id\", \"event_time_ts\", \"amount\"])\n",
    "    print(\"DQ duplicate count for operational: \" + str(dup_count))\n",
    "\n",
    "    silver_path = os.path.join(config[\"storage\"][\"silver_path\"], \"operational_cleaned\")\n",
    "\n",
    "    (\n",
    "        df_clean.write\n",
    "        .mode(\"overwrite\")\n",
    "        .parquet(silver_path)\n",
    "    )\n",
    "\n",
    "    log_lineage(config, \"silver_operational_cleaning\", [bronze_path], silver_path)\n",
    "    return silver_path\n",
    "\n",
    "def integrate_sources_to_silver(spark, config, batch_id):\n",
    "    op_path = os.path.join(config[\"storage\"][\"silver_path\"], \"operational_cleaned\")\n",
    "    logs_bronze = os.path.join(config[\"storage\"][\"bronze_path\"], \"logs\")\n",
    "    external_bronze = os.path.join(config[\"storage\"][\"bronze_path\"], \"external\")\n",
    "\n",
    "    df_op = spark.read.parquet(op_path)\n",
    "    df_logs = spark.read.parquet(logs_bronze)\n",
    "    df_ext = spark.read.parquet(external_bronze)\n",
    "\n",
    "    df_logs_sel = df_logs.select(\"customer_id\", \"log_event_type\", \"log_timestamp\")\n",
    "    df_ext_sel = df_ext.select(\"customer_id\", \"partner_score\", \"partner_segment\")\n",
    "\n",
    "    df_join_1 = df_op.join(df_logs_sel, on=\"customer_id\", how=\"left\")\n",
    "    df_join_all = df_join_1.join(df_ext_sel, on=\"customer_id\", how=\"left\")\n",
    "\n",
    "    silver_int_path = os.path.join(config[\"storage\"][\"silver_path\"], \"integrated\")\n",
    "\n",
    "    (\n",
    "        df_join_all.write\n",
    "        .mode(\"overwrite\")\n",
    "        .parquet(silver_int_path)\n",
    "    )\n",
    "\n",
    "    log_lineage(config, \"silver_integration\", [op_path, logs_bronze, external_bronze], silver_int_path)\n",
    "    return silver_int_path\n",
    "\n",
    "def run_silver_processing():\n",
    "    config = load_config()\n",
    "    spark = create_spark_session(\n",
    "        config[\"spark\"][\"app_name\"] + \"_silver\",\n",
    "        master=config[\"spark\"][\"master\"],\n",
    "    )\n",
    "\n",
    "    batch_id = config[\"batch\"][\"training_quarter\"]\n",
    "\n",
    "    clean_operational(spark, config, batch_id)\n",
    "    integrate_sources_to_silver(spark, config, batch_id)\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_silver_processing()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Gold layer – features, labels, training sets – `src/gold_feature_store.py`\n",
    "\n",
    "Implements quarterly snapshotting, label generation, and train/eval splits.\n",
    "\n",
    "Assumption:\n",
    "- A churn-like task: label = 1 if customer has no events in next N days after a reference date, else 0.\n",
    "- Features: aggregated spend, counts, and partner_score.\n",
    "\n",
    "```python\n",
    "# src/gold_feature_store.py\n",
    "# Gold layer: ML-ready features, labels, and training sets for quarterly retraining.\n",
    "\n",
    "import os\n",
    "from pyspark.sql.functions import col, sum as _sum, countDistinct, max as _max, when, datediff\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from config_loader import load_config\n",
    "from utils import create_spark_session, log_lineage\n",
    "\n",
    "def build_feature_table(spark, config, batch_id):\n",
    "    silver_int_path = os.path.join(config[\"storage\"][\"silver_path\"], \"integrated\")\n",
    "    df = spark.read.parquet(silver_int_path)\n",
    "\n",
    "    agg = (\n",
    "        df.groupBy(\"customer_id\")\n",
    "        .agg(\n",
    "            _sum(\"amount\").alias(\"total_amount\"),\n",
    "            countDistinct(\"event_time_ts\").alias(\"active_days\"),\n",
    "            _max(\"event_time_ts\").alias(\"last_activity_ts\"),\n",
    "            _max(\"partner_score\").alias(\"partner_score_max\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    gold_feat_path = os.path.join(config[\"storage\"][\"gold_path\"], \"features\", batch_id)\n",
    "\n",
    "    (\n",
    "        agg.write\n",
    "        .mode(\"overwrite\")\n",
    "        .parquet(gold_feat_path)\n",
    "    )\n",
    "\n",
    "    log_lineage(config, \"gold_feature_table\", [silver_int_path], gold_feat_path)\n",
    "    return gold_feat_path\n",
    "\n",
    "def build_labels(spark, config, batch_id, prediction_window_days=90):\n",
    "    silver_int_path = os.path.join(config[\"storage\"][\"silver_path\"], \"integrated\")\n",
    "    df = spark.read.parquet(silver_int_path)\n",
    "\n",
    "    ref_date = config[\"batch\"][\"cutoff_date\"]\n",
    "\n",
    "    window_spec = Window.partitionBy(\"customer_id\")\n",
    "\n",
    "    df_ref = (\n",
    "        df.withColumn(\"last_activity_ts\", _max(\"event_time_ts\").over(window_spec))\n",
    "        .select(\"customer_id\", \"last_activity_ts\")\n",
    "        .dropDuplicates([\"customer_id\"])\n",
    "    )\n",
    "\n",
    "    df_label = df_ref.withColumn(\n",
    "        \"label\",\n",
    "        when(\n",
    "            datediff(ref_date, col(\"last_activity_ts\")) > prediction_window_days,\n",
    "            1,\n",
    "        ).otherwise(0),\n",
    "    )\n",
    "\n",
    "    gold_label_path = os.path.join(config[\"storage\"][\"gold_path\"], \"labels\", batch_id)\n",
    "\n",
    "    (\n",
    "        df_label.write\n",
    "        .mode(\"overwrite\")\n",
    "        .parquet(gold_label_path)\n",
    "    )\n",
    "\n",
    "    log_lineage(config, \"gold_labels\", [silver_int_path], gold_label_path)\n",
    "    return gold_label_path\n",
    "\n",
    "def build_training_set(spark, config, batch_id):\n",
    "    feat_path = os.path.join(config[\"storage\"][\"gold_path\"], \"features\", batch_id)\n",
    "    label_path = os.path.join(config[\"storage\"][\"gold_path\"], \"labels\", batch_id)\n",
    "\n",
    "    df_feat = spark.read.parquet(feat_path)\n",
    "    df_label = spark.read.parquet(label_path)\n",
    "\n",
    "    df_train = df_feat.join(df_label, on=\"customer_id\", how=\"inner\")\n",
    "\n",
    "    train_path = os.path.join(config[\"storage\"][\"gold_path\"], \"training_sets\", batch_id)\n",
    "\n",
    "    (\n",
    "        df_train.write\n",
    "        .mode(\"overwrite\")\n",
    "        .parquet(train_path)\n",
    "    )\n",
    "\n",
    "    log_lineage(config, \"gold_training_set\", [feat_path, label_path], train_path)\n",
    "    return train_path\n",
    "\n",
    "def run_gold_pipeline():\n",
    "    config = load_config()\n",
    "    spark = create_spark_session(\n",
    "        config[\"spark\"][\"app_name\"] + \"_gold\",\n",
    "        master=config[\"spark\"][\"master\"],\n",
    "    )\n",
    "\n",
    "    batch_id = config[\"batch\"][\"training_quarter\"]\n",
    "\n",
    "    feat_path = build_feature_table(spark, config, batch_id)\n",
    "    label_path = build_labels(spark, config, batch_id)\n",
    "    train_path = build_training_set(spark, config, batch_id)\n",
    "\n",
    "    print(\"Feature table at \" + feat_path)\n",
    "    print(\"Labels at \" + label_path)\n",
    "    print(\"Training set at \" + train_path)\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_gold_pipeline()\n",
    "```\n",
    "\n",
    "## 9. Orchestration driver – `src/run_quarterly_pipeline.py`\n",
    "\n",
    "This script glues bronze → silver → gold into a single quarterly run. In the report, this corresponds to the “orchestration” piece this can later be mapped to Airflow DAG or similar.\n",
    "\n",
    "```python\n",
    "# src/run_quarterly_pipeline.py\n",
    "# Simple quarterly orchestration: run bronze, silver, and gold phases in sequence.\n",
    "\n",
    "from bronze_ingestion import run_bronze_ingestion\n",
    "from silver_processing import run_silver_processing\n",
    "from gold_feature_store import run_gold_pipeline\n",
    "\n",
    "def run_full_quarterly_pipeline():\n",
    "    print(\"Starting quarterly batch pipeline...\")\n",
    "    run_bronze_ingestion()\n",
    "    run_silver_processing()\n",
    "    run_gold_pipeline()\n",
    "    print(\"Quarterly batch pipeline completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_full_quarterly_pipeline()\n",
    "```\n",
    "\n",
    "## 10. Minimal `README.md` outline\n",
    "\n",
    "```markdown\n",
    "# Batch-Processing Data Architecture for Quarterly ML Retraining\n",
    "\n",
    "This project implements the conceptual bronze–silver–gold batch-processing architecture described in the report:\n",
    "\n",
    "- **Bronze**: Immutable, append-only raw data lake zone for operational databases, logs, and external feeds.\n",
    "- **Silver**: Cleaned, deduplicated, and integrated data with enforced schemas and basic data quality checks.\n",
    "- **Gold**: ML-ready feature tables, labels, and quarterly training datasets.\n",
    "\n",
    "## Running\n",
    "\n",
    "1. Create and populate `data/raw_sources/operational`, `logs`, `external` with CSVs.\n",
    "2. Adjust `config/config.yaml` (paths, cutoff date, batch id).\n",
    "3. Install dependencies:\n",
    "\n",
    "   ```bash\n",
    "   pip install -r requirements.txt\n",
    "   ```\n",
    "\n",
    "4. Run the full quarterly pipeline:\n",
    "\n",
    "   ```bash\n",
    "   python -m src.run_quarterly_pipeline\n",
    "   ``"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
